'''

When you run a SQL job
• Spark knows the DF dependencies in advance – unresolved logical transformation plan
• Catalyst resolves references and expression types – resolved logical plan
• Catalyst compresses and pattern matches on the plan tree – optimized logical plan
• Catalyst generates physical execution plans




'''
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.config("spark.master", "local") \
    .appName("Reading Query Plans") \
    .getOrCreate()

sc = spark.sparkContext

# 1 - a simple transformation
simpleNumbers = spark.range(1, 1000000)

times5 = simpleNumbers.selectExpr("id * 5 as id")
times5.explain()  # this is how you show

'''
    == Physical Plan ==
    *(1) Project [(id#0L * 5) AS id#2L]
    +- *(1) Range (1, 1000000, step=1, splits=6)
'''

# plan 2 - a shuffle

moreNumbers = spark.range(1, 1000000, 2)

split7 = moreNumbers.repartition(7)

split7.explain()
'''
== Physical
Plan ==
Exchange
RoundRobinPartitioning(7), false, [id =  # 16]
+- * (1)
Range(1, 1000000, step=2, splits=6)

'''

# plan 3 - shuffle + transformation
split7.selectExpr("id * 5 as id").explain()

'''
== Physical
Plan ==
*(2)
Project[(id  # 4L * 5) AS id#8L]
         + - Exchange RoundRobinPartitioning(7), false, [id =  # 29]
+- * (1)
Range(1, 1000000, step=2, splits=6)

'''

# plan 4 - a more complex job with a join

ds1 = spark.range(1, 10000000)

ds2 = spark.range(1, 20000000, 2)

ds3 = ds1.repartition(7)

ds4 = ds2.repartition(9)

ds5 = ds3.selectExpr("id * 3 as id")

joined = ds5.join(ds4, "id")

sum = joined.selectExpr("sum(id)")
sum.explain()

'''
== Physical Plan ==
  *(7) HashAggregate(keys=[], functions=[sum(id#18L)])
  +- Exchange SinglePartition, true, [id=#99]
    +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#18L)])
      +- *(6) Project [id#18L]
        +- *(6) SortMergeJoin [id#18L], [id#12L], Inner
          :- *(3) Sort [id#18L ASC NULLS FIRST], false, 0
          :  +- Exchange hashpartitioning(id#18L, 200), true, [id=#83]
          :     +- *(2) Project [(id#10L * 3) AS id#18L]
          :        +- Exchange RoundRobinPartitioning(7), false, [id=#79]
          :           +- *(1) Range (1, 10000000, step=1, splits=6)
          +- *(5) Sort [id#12L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(id#12L, 200), true, [id=#90]
              +- Exchange RoundRobinPartitioning(9), false, [id=#89]
                +- *(4) Range (1, 20000000, step=2, splits=6)

'''

employeesDF = spark.read.option("header", True).csv("/tmp/employees_headers.csv")
empEur = employeesDF.selectExpr("firstName", "lastName", "salary / 1.1 as salary_EUR")

'''
== PhysicalPlan ==
*(1)Project[firstName# 153, lastName#155, (cast(salary#159 as double) / 1.1) AS salary_EUR#168]
            + - * (1)FileScancsv[firstName  # 153,lastName#155,salary#159] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/employees_headers.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<firstName:string,lastName:string,salary:string>
'''

avgSals = employeesDF \
    .selectExpr("dept", "cast(salary as int) as salary") \
    .groupBy("dept") \
    .avg("salary")

'''

 == Physical Plan ==
  *(2) HashAggregate(keys=[dept#156], functions=[avg(cast(salary#181 as bigint))])
    +- Exchange hashpartitioning(dept#156, 200)
      +- *(1) HashAggregate(keys=[dept#156], functions=[partial_avg(cast(salary#181 as bigint))])
        +- *(1) Project [dept#156, cast(salary#159 as int) AS salary#181]
          +- *(1) FileScan csv [dept#156,salary#159] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/employees_headers.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dept:string,salary:string>
'''

d1 = spark.range(1, 10000000, 3)

d2 = spark.range(1, 10000000, 5)
j1 = d1.join(d2, "id")

'''

== Physical Plan ==
  *(5) Project [id#195L]
    +- *(5) SortMergeJoin [id#195L], [id#197L], Inner
      :- *(2) Sort [id#195L ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(id#195L, 200)
      :     +- *(1) Range (1, 10000000, step=3, splits=6)
      +- *(4) Sort [id#197L ASC NULLS FIRST], false, 0
        +- Exchange hashpartitioning(id#197L, 200)
          +- *(3) Range (1, 10000000, step=5, splits=6)
'''

'''
Query Plans
A query plan
• describes all the operations Spark will execute when the action is triggered
• has information about partitioning scheme
• has information about the number of partitions in advance
• shows job stages
• is shown by dataFrame.explain()
Explain (true) will give
• the parsed logical plan
• the analyzed logical plan
• the optimized logical plan (via Catalyst)
• the physical execution plan (generated by Catalyst)
Spark works like a compiler


Query plans = layout of Spark computations (before they run)
Whenever you see "exchange", that's a shuffle
Number of shuffles = number of stages
Number of tasks = number of partitions of each intermediate DF
Sometimes Spark already optimizes some plans!

'''
